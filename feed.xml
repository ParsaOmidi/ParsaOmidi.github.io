<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://parsaomidi.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://parsaomidi.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-02-27T03:54:12+00:00</updated><id>https://parsaomidi.github.io/feed.xml</id><title type="html">blank</title><subtitle>Personal Website. </subtitle><entry><title type="html">Sentiment Analysis with Statistical Methods</title><link href="https://parsaomidi.github.io/blog/2023/sentiment-analysis-with-statistical-ml/" rel="alternate" type="text/html" title="Sentiment Analysis with Statistical Methods"/><published>2023-02-12T01:00:00+00:00</published><updated>2023-02-12T01:00:00+00:00</updated><id>https://parsaomidi.github.io/blog/2023/sentiment-analysis-with-statistical-ml</id><content type="html" xml:base="https://parsaomidi.github.io/blog/2023/sentiment-analysis-with-statistical-ml/"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/sentiment-statistical-01-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/sentiment-statistical-01-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/sentiment-statistical-01-1400.webp"/> <img src="/assets/img/sentiment-statistical-01.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <blockquote> <p><a href="https://github.com/josumsc/classic-ml-sentiment-analysis/blob/master/src/IMDB_Sentiment_Analysis.ipynb">This repository</a> shows a quick implementation of the concepts in this article.</p> </blockquote> <p>Given all the fuss about chatbots, large language models (LLMs) and Generative Models, I was one of those excited ML practitioners that decided to give NLP a deeper look than before and see if I could apply all those promising technologies to my career and create something useful.</p> <p>So I enrolled on the <a href="https://www.deeplearning.ai/courses/natural-language-processing-specialization/">Deep Learning Specialization by DeepLearning.ai</a> expecting to see how to speak with a machine to make it do my job for me. But, to my surprise, the course didn’t just jump straight ahead to those fancy technologies, but instead, it started with the basics: sentiment analysis and statistical learning.</p> <p>It was a revelation to see these concepts again after my Master’s degree. I had forgotten how simple and powerful they are when dealing with day-to-day tasks and to establish baselines that are sometimes really difficult to beat. So I decided to write this article to share my experience and hopefully help someone else to get started with NLP.</p> <h2 id="sentiment-analysis">Sentiment Analysis</h2> <p>Sentiment analysis is the task of classifying a text into a positive or negative sentiment. It is a very common task in NLP and it is a good starting point to learn about the field. In teams dealing with Customer Service, for example, sentiment analysis is used to classify customer reviews and complaints into positive or negative. This way, the team can focus on the negative reviews and improve the customer experience. In politics or brand management departments, it can be used to leverage the public opinion about a topic using social media posts.</p> <p>Sentiment analysis is one of the most basic tasks in NLP, and for that reason it’s currently widespread in industry as well as in academia. For these reasons it is a good benchmarking tool to compare different models and techniques. The current state-of-the-art is focused on using LLMs, big models trained using a lot of data and compute power. These models can often be downloaded using public hubs as <a href="https://huggingface.co/">HuggingFace</a> and require little to none finetuning to be used. However, when the task at hand is too specific that we cannot find an appropriate pre-trained model or the task is too small to justify the use of a big model, we can use statistical methods to perform sentiment analysis.</p> <p>In this article, we will use the <a href="https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews">IMDB dataset</a> from Kaggle, which contains 50,000 movie reviews from IMDB, labeled as positive or negative.</p> <h2 id="statistical-methods">Statistical Methods</h2> <p>Among the different statistical methods that can be used to perform sentiment analysis, we will focus on the following:</p> <ul> <li><strong>Logistic Regression</strong>, a linear model that uses the sigmoid function to fit the parameters to a binary classification. This binary feature can be surpassed using <a href="https://machinelearningmastery.com/one-vs-rest-and-one-vs-one-for-multi-class-classification/">One-vs-All or similar techniques</a> in order to fit it to K classes.</li> <li><strong>Naive Bayes</strong>, a probabilistic model that uses Bayes’ theorem to calculate the probability of a class given a set of features. It is a very simple model that can be used to perform sentiment analysis in a very fast way.</li> </ul> <p>These methods are very simple and easy to implement, and they can be used as a baseline to compare with more complex models. In addition, they have been proven to be very effective in sentiment analysis, as they were the state-of-the-art alongside SVMs in the early 2000s.</p> <h2 id="data-preparation">Data Preparation</h2> <p>In every NLP pipeline there are different steps that need to be performed before feeding the data to the model. This is necessary as our models don’t understand the strings that compose our data, but rather numbers. So we need to transform our data into a format that our models can understand. Also, we can use this step to clean our data and remove unnecessary information, such as blank spaces, punctuation or stopwords (as prepositions and very common words).</p> <p>Apart from the tokenization (convert text to numbers) and the cleansing of noisy characters, we may also stem our words. Stemming is the process of reducing a word to its root form. For example, the words “running”, “runs”, “ran” and “run” would be reduced to “run”. This is useful as it reduces the number of features that our model needs to learn, and thus it also helps to reduce the noise in our data.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="https://devopedia.org/images/article/218/8583.1569386710-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="https://devopedia.org/images/article/218/8583.1569386710-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="https://devopedia.org/images/article/218/8583.1569386710-1400.webp"/> <img src="https://devopedia.org/images/article/218/8583.1569386710.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>In a nutshell, our pipeline will look like this:</p> <ol> <li>Lowering the text, so uppercase and lowercase letters are considered the same and we don’t have to deal with different representations of the same word.</li> <li>Tokenization, using blank spaces as separators we separate our strings into tokens that we may look up in a learnt mapping of <code class="language-plaintext highlighter-rouge">{string: integer}</code> often called <em>vocabulary</em>.</li> <li>Removing punctuation and stopwords, to reduce the noise.</li> <li>Stemming the words to their root form thanks to the <a href="https://tartarus.org/martin/PorterStemmer/">Porter Stemmer</a>, to reduce the number of features.</li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">preprocess_text</span><span class="p">(</span>
    <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">stopwords</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="c1"># Lowercase text
</span>    <span class="n">preprocessed_text</span> <span class="o">=</span> <span class="n">text</span><span class="p">.</span><span class="nf">lower</span><span class="p">()</span>

    <span class="c1"># Tokenize text
</span>    <span class="n">preprocessed_text</span> <span class="o">=</span> <span class="nf">word_tokenize</span><span class="p">(</span><span class="n">preprocessed_text</span><span class="p">)</span>

    <span class="c1"># Remove punctuation
</span>    <span class="n">preprocessed_text</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">preprocessed_text</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">string</span><span class="p">.</span><span class="n">punctuation</span><span class="p">]</span>

    <span class="c1"># Remove stopwords
</span>    <span class="n">preprocessed_text</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">preprocessed_text</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stopwords</span><span class="p">]</span>

    <span class="c1"># Stem text
</span>    <span class="n">stemmer</span> <span class="o">=</span> <span class="nc">PorterStemmer</span><span class="p">()</span>
    <span class="n">preprocessed_text</span> <span class="o">=</span> <span class="p">[</span><span class="n">stemmer</span><span class="p">.</span><span class="nf">stem</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">preprocessed_text</span><span class="p">]</span>

    <span class="k">return</span> <span class="s">' '</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">preprocessed_text</span><span class="p">)</span>
</code></pre></div></div> <hr/> <h2 id="modelling">Modelling</h2> <h3 id="logistic-regression">Logistic Regression</h3> <p>The first step in implementing this method is to create a frequency table of the appearance of each token in the different classes. In this case, we will have two records for each token: one for the positive class and one for the negative class. This table will be used to calculate the probability of a class given a token, that we can later on aggregate to the probability of a class given a set of tokens.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_word_dict</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="nf">word_tokenize</span><span class="p">(</span><span class="n">sentence</span><span class="p">):</span>
            <span class="c1"># define the key, which is the word and label tuple
</span>            <span class="n">pair</span> <span class="o">=</span> <span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
            
            <span class="c1"># if the key exists in the dictionary, increment the count
</span>            <span class="k">if</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">result</span><span class="p">:</span>
                <span class="n">result</span><span class="p">[</span><span class="n">pair</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

            <span class="c1"># else, if the key is new, add it to the dictionary and set the count to 1
</span>            <span class="k">else</span><span class="p">:</span>
                <span class="n">result</span><span class="p">[</span><span class="n">pair</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="k">return</span> <span class="n">result</span>
</code></pre></div></div> <p>After creating the frequency table, we can then transform our sentences into a matrix of $N*K$ dimensions, where $N$ is the number of sentences and $K$ is the number of classes in our problem, which will represent the sum of appearances of each token in each class.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">extract_features</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">freqs</span><span class="p">,</span> <span class="n">preprocess</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">preprocess</span><span class="p">:</span>
      <span class="n">word_l</span> <span class="o">=</span> <span class="nf">preprocess_text</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">stopwords</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">word_l</span> <span class="o">=</span> <span class="nf">word_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
    <span class="c1">#bias term is set to 1
</span>    <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> 
    
    <span class="c1"># loop through each token
</span>    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">word_l</span><span class="p">:</span>
        <span class="nf">if </span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span> <span class="ow">in</span> <span class="n">freqs</span><span class="p">.</span><span class="nf">keys</span><span class="p">():</span>
            <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="n">freqs</span><span class="p">[(</span><span class="n">word</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)]</span>
        <span class="nf">if </span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span> <span class="ow">in</span> <span class="n">freqs</span><span class="p">.</span><span class="nf">keys</span><span class="p">():</span>
            <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">+=</span> <span class="n">freqs</span><span class="p">[(</span><span class="n">word</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)]</span>
    <span class="k">return</span> <span class="n">x</span>


<span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="nf">len</span><span class="p">(</span><span class="n">preprocessed_train</span><span class="p">),</span> <span class="mi">3</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">preprocessed_train</span><span class="p">.</span><span class="n">values</span><span class="p">):</span>
    <span class="n">X_train</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span><span class="o">=</span> <span class="nf">extract_features</span><span class="p">(</span><span class="n">row</span><span class="p">,</span> <span class="n">freqs</span><span class="p">)</span>

<span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="nf">len</span><span class="p">(</span><span class="n">preprocessed_test</span><span class="p">),</span> <span class="mi">3</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">preprocessed_test</span><span class="p">.</span><span class="n">values</span><span class="p">):</span>
    <span class="n">X_test</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span><span class="o">=</span> <span class="nf">extract_features</span><span class="p">(</span><span class="n">row</span><span class="p">,</span> <span class="n">freqs</span><span class="p">)</span>
</code></pre></div></div> <p>Now we just have to fit our <code class="language-plaintext highlighter-rouge">LogisticRegression</code> model, which will learn a bias parameter (depending on the proportion of each class in the train corpus) and the weights of each feature (the sum of appearances of each token in each class). For this model we will use the <code class="language-plaintext highlighter-rouge">sklearn</code> implementation:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="n">log_model</span> <span class="o">=</span> <span class="nc">LogisticRegression</span><span class="p">()</span>
<span class="n">log_model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div></div> <h3 id="naive-bayes">Naive Bayes</h3> <p>As a second traditional method, we select <a href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier">Naive Bayes</a>. This method is based on the conditional probability of an example to belong to a certain class given the probabilities of each of the tokens, which will be summarized in what’s called <em>loglikelihoods</em>. To calculate this probability, we will be using the same frequency table that we calculated for the Logistic Regression:</p> \[P(W_{pos}) = \frac{freq_{pos} + 1}{N_{pos} + V}\] \[P(W_{neg}) = \frac{freq_{neg} + 1}{N_{neg} + V}\] \[\text{loglikelihood} = \log \left(\frac{P(W_{pos})}{P(W_{neg})} \right)\] <p>Besides, a bias term is fitted which represents the overall likelihood of an example of a certain class to be draft from the corpus. This bias term is called <em>logprior</em>.</p> \[\text{logprior} = \log (D_{pos}) - \log (D_{neg})\] <p>Finally, after the calculation of these both terms we just need to add them to form the final probability of an example to belong to a particular class, and then perform a <em>softmax</em> operation to retrieve the index with the higher probability. As in this case the problem is binary, we can summarize the equation as:</p> \[p = logprior + \sum_i^N (loglikelihood_i)\] <p>Although there are several implementations of Naive Bayes in Python, given the relative simpleness of this case we can create our own functions in order to get a better grasp of the inner workings of the algorithm.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train_naive_bayes</span><span class="p">(</span><span class="n">freqs</span><span class="p">,</span> <span class="n">train_x</span><span class="p">,</span> <span class="n">train_y</span><span class="p">):</span>
    <span class="n">loglikelihood</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">logprior</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="n">vocab</span> <span class="o">=</span> <span class="p">{</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">freqs</span><span class="p">.</span><span class="nf">keys</span><span class="p">()}</span>
    <span class="n">V</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>

    <span class="c1"># calculate N_pos, N_neg, V_pos, V_neg
</span>    <span class="n">N_pos</span> <span class="o">=</span> <span class="n">N_neg</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">freqs</span><span class="p">.</span><span class="nf">keys</span><span class="p">():</span>
        <span class="c1"># if the label is positive (greater than zero)
</span>        <span class="k">if</span> <span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">N_pos</span> <span class="o">+=</span> <span class="n">freqs</span><span class="p">[</span><span class="n">pair</span><span class="p">]</span>
        <span class="c1"># else, the label is negative
</span>        <span class="k">else</span><span class="p">:</span>
            <span class="n">N_neg</span> <span class="o">+=</span> <span class="n">freqs</span><span class="p">[</span><span class="n">pair</span><span class="p">]</span>
    
    <span class="n">D</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">train_x</span><span class="p">)</span>
    <span class="n">D_pos</span> <span class="o">=</span> <span class="n">train_y</span><span class="p">[</span><span class="n">train_y</span> <span class="o">==</span> <span class="mi">1</span><span class="p">].</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">D_neg</span> <span class="o">=</span> <span class="n">train_y</span><span class="p">[</span><span class="n">train_y</span> <span class="o">==</span> <span class="mi">0</span><span class="p">].</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># Calculate logprior
</span>    <span class="n">logprior</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">D_pos</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">D_neg</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">vocab</span><span class="p">:</span>
        <span class="c1"># get the positive and negative frequency of the word
</span>        <span class="n">freq_pos</span> <span class="o">=</span> <span class="n">freqs</span><span class="p">[</span><span class="n">word</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]</span> <span class="nf">if </span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span> <span class="ow">in</span> <span class="n">freqs</span><span class="p">.</span><span class="nf">keys</span><span class="p">()</span> <span class="k">else</span> <span class="mi">0</span>
        <span class="n">freq_neg</span> <span class="o">=</span> <span class="n">freqs</span><span class="p">[</span><span class="n">word</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">]</span> <span class="nf">if </span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span> <span class="ow">in</span> <span class="n">freqs</span><span class="p">.</span><span class="nf">keys</span><span class="p">()</span> <span class="k">else</span> <span class="mi">0</span>
        <span class="c1"># calculate the probability that each word is positive, and negative
</span>        <span class="n">p_w_pos</span> <span class="o">=</span> <span class="p">(</span><span class="n">freq_pos</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">N_pos</span> <span class="o">+</span> <span class="n">V</span><span class="p">)</span>
        <span class="n">p_w_neg</span> <span class="o">=</span> <span class="p">(</span><span class="n">freq_neg</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">N_neg</span> <span class="o">+</span> <span class="n">V</span><span class="p">)</span>
        <span class="c1"># calculate the log likelihood of the word
</span>        <span class="n">loglikelihood</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">p_w_pos</span> <span class="o">/</span> <span class="n">p_w_neg</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">logprior</span><span class="p">,</span> <span class="n">loglikelihood</span>


<span class="n">logprior</span><span class="p">,</span> <span class="n">loglikelihood</span> <span class="o">=</span> <span class="nf">train_naive_bayes</span><span class="p">(</span><span class="n">freqs</span><span class="p">,</span> <span class="n">preprocessed_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div></div> <p>Once we get our parameters <code class="language-plaintext highlighter-rouge">logprior</code> and <code class="language-plaintext highlighter-rouge">loglikelihood</code>, that represent the bias and the weights of this model, we can use them to predict the class of a new example by aggregating the scores of their tokens:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">naive_bayes_predict</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">logprior</span><span class="p">,</span> <span class="n">loglikelihood</span><span class="p">):</span>
    <span class="n">word_l</span> <span class="o">=</span> <span class="nf">word_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

    <span class="c1"># start by adding our logprior
</span>    <span class="n">p</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">p</span> <span class="o">+=</span> <span class="n">logprior</span>

    <span class="c1"># words that are not in vocabulary simply get ignored
</span>    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">word_l</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">loglikelihood</span><span class="p">:</span>
            <span class="n">p</span> <span class="o">+=</span> <span class="n">loglikelihood</span><span class="p">[</span><span class="n">word</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">p</span>


<span class="k">def</span> <span class="nf">test_naive_bayes</span><span class="p">(</span><span class="n">test_x</span><span class="p">,</span> <span class="n">test_y</span><span class="p">,</span> <span class="n">logprior</span><span class="p">,</span> <span class="n">loglikelihood</span><span class="p">,</span> <span class="n">naive_bayes_predict</span><span class="o">=</span><span class="n">naive_bayes_predict</span><span class="p">):</span>
    <span class="n">y_hats</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">test_x</span><span class="p">:</span>
        <span class="k">if</span> <span class="nf">naive_bayes_predict</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">logprior</span><span class="p">,</span> <span class="n">loglikelihood</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">y_hat_i</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">y_hat_i</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">y_hats</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">y_hat_i</span><span class="p">)</span>

    <span class="n">error</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">abs</span><span class="p">(</span><span class="n">test_y</span> <span class="o">-</span> <span class="n">y_hats</span><span class="p">))</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">error</span>
    <span class="k">return</span> <span class="n">y_hats</span><span class="p">,</span> <span class="n">accuracy</span>
</code></pre></div></div> <p>As we can see, this approach is very similar than the one taken in the Logistic Regression, but it is often more efficient in terms of computational cost and in model performance. In fact, in the case of the IMDb dataset, the Naive Bayes model achieves a better accuracy than even a more complex model based on RNNs (<a href="https://github.com/josumsc/classic-ml-sentiment-analysis/blob/master/src/IMDB_Sentiment_Analysis.ipynb">reference</a>).</p> <h2 id="conclusion">Conclusion</h2> <p>As we just saw, the implementation of a simple model based on the frequency of the tokens in the corpus can be easily explained, implemented and trained. Furthermore, their efficiency allow us to fit these models to a large vocabulary of custom tokens to adapt them to our specific needs without the need of a large amount of data.</p> <p>We should not be narrow-minded and think that due to the emergence of far more powerful algorithms these models are not useful anymore, as they both serve as a good starting point to understand our datasets and as a baseline to compare the performance of more complex models.</p>]]></content><author><name></name></author><category term="nlp"/><category term="python"/><summary type="html"><![CDATA[Using statistical methods to perform sentiment analysis on a dataset of reviews might be simpler than you think.]]></summary></entry><entry><title type="html">Face swapping between two images</title><link href="https://parsaomidi.github.io/blog/2023/Face-swapping/" rel="alternate" type="text/html" title="Face swapping between two images"/><published>2023-01-15T15:09:00+00:00</published><updated>2023-01-15T15:09:00+00:00</updated><id>https://parsaomidi.github.io/blog/2023/Face-swapping</id><content type="html" xml:base="https://parsaomidi.github.io/blog/2023/Face-swapping/"><![CDATA[<p>This is a Python script that makes use of the OpenCV and dlib libraries to perform face swapping between two images. The script reads in two images, finds the facial landmarks on each image using dlib or OpenCV’s Haar cascades, aligns the faces using Procrustes analysis, and finally swaps the faces by blending the two images together.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
</pre></td><td class="code"><pre><span class="c1"># Import necessary libraries
</span><span class="kn">import</span> <span class="n">cv2</span>
<span class="kn">import</span> <span class="n">dlib</span>
<span class="kn">import</span> <span class="n">numpy</span>
<span class="kn">from</span> <span class="n">time</span> <span class="kn">import</span> <span class="n">sleep</span>
<span class="kn">import</span> <span class="n">sys</span>

<span class="c1"># Pretrained model that predicts facial feature rectangles
</span><span class="n">PREDICTOR_PATH</span> <span class="o">=</span> <span class="s">"shape_predictor_68_face_landmarks.dat"</span>
<span class="n">SCALE_FACTOR</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">FEATHER_AMOUNT</span> <span class="o">=</span> <span class="mi">11</span>

<span class="c1"># Define different facial feature points for later use
</span><span class="n">FACE_POINTS</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="mi">17</span><span class="p">,</span> <span class="mi">68</span><span class="p">))</span>
<span class="n">MOUTH_POINTS</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="mi">48</span><span class="p">,</span> <span class="mi">61</span><span class="p">))</span>
<span class="n">RIGHT_BROW_POINTS</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="mi">17</span><span class="p">,</span> <span class="mi">22</span><span class="p">))</span>
<span class="n">LEFT_BROW_POINTS</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="mi">22</span><span class="p">,</span> <span class="mi">27</span><span class="p">))</span>
<span class="n">RIGHT_EYE_POINTS</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="mi">36</span><span class="p">,</span> <span class="mi">42</span><span class="p">))</span>
<span class="n">LEFT_EYE_POINTS</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="mi">42</span><span class="p">,</span> <span class="mi">48</span><span class="p">))</span>
<span class="n">NOSE_POINTS</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="mi">27</span><span class="p">,</span> <span class="mi">35</span><span class="p">))</span>
<span class="n">JAW_POINTS</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">17</span><span class="p">))</span>

<span class="c1"># Points used to align images
</span><span class="n">ALIGN_POINTS</span> <span class="o">=</span> <span class="p">(</span><span class="n">LEFT_BROW_POINTS</span> <span class="o">+</span> <span class="n">RIGHT_EYE_POINTS</span> <span class="o">+</span> <span class="n">LEFT_EYE_POINTS</span> <span class="o">+</span>
                               <span class="n">RIGHT_BROW_POINTS</span> <span class="o">+</span> <span class="n">NOSE_POINTS</span> <span class="o">+</span> <span class="n">MOUTH_POINTS</span><span class="p">)</span>

<span class="c1"># Points from the second image to overlay on the first
</span><span class="n">OVERLAY_POINTS</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">LEFT_EYE_POINTS</span> <span class="o">+</span> <span class="n">RIGHT_EYE_POINTS</span> <span class="o">+</span> <span class="n">LEFT_BROW_POINTS</span> <span class="o">+</span> <span class="n">RIGHT_BROW_POINTS</span><span class="p">,</span>
    <span class="n">NOSE_POINTS</span> <span class="o">+</span> <span class="n">MOUTH_POINTS</span><span class="p">,</span>
<span class="p">]</span>

<span class="c1"># Amount of blur to use during colour correction, as a fraction of the
# pupillary distance.
</span><span class="n">COLOUR_CORRECT_BLUR_FRAC</span> <span class="o">=</span> <span class="mf">0.6</span>

<span class="c1"># Load the face cascade classifier and the dlib frontal face detector
</span><span class="n">cascade_path</span><span class="o">=</span><span class="s">'Haarcascades/haarcascade_frontalface_default.xml'</span>
<span class="n">cascade</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nc">CascadeClassifier</span><span class="p">(</span><span class="n">cascade_path</span><span class="p">)</span>
<span class="n">detector</span> <span class="o">=</span> <span class="n">dlib</span><span class="p">.</span><span class="nf">get_frontal_face_detector</span><span class="p">()</span>
<span class="n">predictor</span> <span class="o">=</span> <span class="n">dlib</span><span class="p">.</span><span class="nf">shape_predictor</span><span class="p">(</span><span class="n">PREDICTOR_PATH</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">get_landmarks</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="n">dlibOn</span><span class="p">):</span>
    <span class="s">"""
    Get facial landmarks from the image using either dlib or OpenCV.

    :param im: Input image to get landmarks from.
    :param dlibOn: Boolean indicating whether to use dlib or OpenCV for facial detection.
    :return: Matrix containing the x and y coordinates of the facial landmarks.
    """</span>
    <span class="c1"># If using dlib, detect facial landmarks using the dlib frontal face detector
</span>    <span class="nf">if </span><span class="p">(</span><span class="n">dlibOn</span> <span class="o">==</span> <span class="bp">True</span><span class="p">):</span>
        <span class="n">rects</span> <span class="o">=</span> <span class="nf">detector</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">rects</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span> <span class="s">"error"</span>
        <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">rects</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="s">"error"</span>
        <span class="k">return</span> <span class="n">numpy</span><span class="p">.</span><span class="nf">matrix</span><span class="p">([[</span><span class="n">p</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">.</span><span class="n">y</span><span class="p">]</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="nf">predictor</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="n">rects</span><span class="p">[</span><span class="mi">0</span><span class="p">]).</span><span class="nf">parts</span><span class="p">()])</span>

    <span class="c1"># Otherwise, use OpenCV to detect facial landmarks
</span>    <span class="k">else</span><span class="p">:</span>
        <span class="n">rects</span> <span class="o">=</span> <span class="n">cascade</span><span class="p">.</span><span class="nf">detectMultiScale</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="mf">1.3</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
        <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">rects</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span> <span class="s">"error"</span>
        <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">rects</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="s">"error"</span>
        <span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">w</span><span class="p">,</span><span class="n">h</span> <span class="o">=</span><span class="n">rects</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">rect</span><span class="o">=</span><span class="n">dlib</span><span class="p">.</span><span class="nf">rectangle</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">x</span><span class="o">+</span><span class="n">w</span><span class="p">,</span><span class="n">y</span><span class="o">+</span><span class="n">h</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">numpy</span><span class="p">.</span><span class="nf">matrix</span><span class="p">([[</span><span class="n">p</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">.</span><span class="n">y</span><span class="p">]</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="nf">predictor</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="n">rect</span><span class="p">).</span><span class="nf">parts</span><span class="p">()])</span>


<span class="k">def</span> <span class="nf">annotate_landmarks</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="n">landmarks</span><span class="p">):</span>
    <span class="s">"""
    Annotate facial landmarks on the image.

    :param im: Input image to annotate landmarks on.
    :param landmarks: Matrix containing the x and y coordinates of the facial landmarks.
    :return: Copy of the input image with facial landmarks annotated.
    """</span>
    <span class="n">im</span> <span class="o">=</span> <span class="n">im</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">point</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">landmarks</span><span class="p">):</span>
        <span class="n">pos</span> <span class="o">=</span> <span class="p">(</span><span class="n">point</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">point</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
        <span class="n">cv2</span><span class="p">.</span><span class="nf">putText</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="nf">str</span><span class="p">(</span><span class="n">idx</span><span class="p">),</span> <span class="n">pos</span><span class="p">,</span>
                    <span class="n">fontFace</span><span class="o">=</span><span class="n">cv2</span><span class="p">.</span><span class="n">FONT</span>
</pre></td></tr></tbody></table></code></pre></figure>]]></content><author><name></name></author><category term="MachineVision"/><category term="OpenCV"/><category term="Python"/><summary type="html"><![CDATA[Using statistical methods to perform sentiment analysis on a dataset of reviews might be simpler than you think.]]></summary></entry><entry><title type="html">Streamlit your Data Science application into AWS</title><link href="https://parsaomidi.github.io/blog/2023/streamlit-your-application-into-aws/" rel="alternate" type="text/html" title="Streamlit your Data Science application into AWS"/><published>2023-01-14T01:00:00+00:00</published><updated>2023-01-14T01:00:00+00:00</updated><id>https://parsaomidi.github.io/blog/2023/streamlit-your-application-into-aws</id><content type="html" xml:base="https://parsaomidi.github.io/blog/2023/streamlit-your-application-into-aws/"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/streamlit-aws-01-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/streamlit-aws-01-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/streamlit-aws-01-1400.webp"/> <img src="/assets/img/streamlit-aws-01.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <blockquote> <p>To check a more advanced real world application developed using these guidelines please consider visiting <a href="https://github.com/josumsc/twitter-sentiment-analysis">this repository</a></p> </blockquote> <p>Have you ever seen those fancy Data Science web apps that include interactive plots, Machine Learning inference in realtime and custom user inputs? Well, I have and I was always wondering how they were built. I mean, I’m sure that a whole team of front-end engineers, data engineers and data scientists could create something like that, but I was always looking for a simpler solution. And then I found <a href="https://docs.streamlit.io">Streamlit</a>.</p> <h2 id="what-is-streamlit">What is Streamlit?</h2> <p>Streamlit is a Python framework that allows you to deploy Data Science applications in the form of Python scripts, dealing with the back and most of the front-end nuances for you. It’s a great tool for Data Scientists that want to share their work with the world, but don’t want to spend too much time dealing with web development.</p> <p>It also includes a Cloud service to host your applications, which is great for a personal project or a small team. But what if you want to deploy your application to a bigger audience or you are concerned about privacy and security issues? Well, you can do that too, but you’ll need to use AWS, which we will talk about later.</p> <h2 id="how-does-it-work">How does it work?</h2> <p>Streamlit is a Python framework, so you’ll need to install it in your environment. You can do that by running the following command:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>streamlit
</code></pre></div></div> <p>If we run <code class="language-plaintext highlighter-rouge">streamlit hello</code> we will be able to see how we have a running web application on our localhost, and with 0 code! Sure enough, our custom Data Science applications will not be so easy to create or simple to use, but the key elements are there.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/streamlit-aws-02-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/streamlit-aws-02-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/streamlit-aws-02-1400.webp"/> <img src="/assets/img/streamlit-aws-02.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="creating-a-streamlit-application">Creating a Streamlit application</h2> <p>There are several commands that we can use to create a Streamlit application, being the most important ones explained in the <a href="https://docs.streamlit.io/library/cheatsheet">Cheat Sheet</a>. As we will progress building our app, I’ll leave some comments in the code to explain what is going on:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">streamlit</span> <span class="k">as</span> <span class="n">st</span>
<span class="kn">import</span> <span class="n">plotly.express</span> <span class="k">as</span> <span class="n">px</span>

<span class="c1"># Sets the page layout to wide and creates a title
</span><span class="n">st</span><span class="p">.</span><span class="nf">set_page_config</span><span class="p">(</span><span class="s">"My dummy Streamlit App"</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="s">"wide"</span><span class="p">)</span>
<span class="c1"># Creates a h1 title
</span><span class="n">st</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="s">"My first Streamlit application"</span><span class="p">)</span>

<span class="c1"># Creates a DataFrame
</span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">({</span>
    <span class="s">'Seller'</span><span class="p">:</span> <span class="p">[</span><span class="s">"A"</span><span class="p">,</span> <span class="s">"B"</span><span class="p">,</span> <span class="s">"C"</span><span class="p">,</span> <span class="s">"D"</span><span class="p">],</span>
    <span class="s">'Sales'</span><span class="p">:</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">]</span>
<span class="p">})</span>

<span class="c1"># Creates a sidebar with a multiselect widget
</span><span class="n">sellers_to_filter</span> <span class="o">=</span> <span class="n">st</span><span class="p">.</span><span class="n">sidebar</span><span class="p">.</span><span class="nf">multiselect</span><span class="p">(</span>
    <span class="s">"Select sellers to filterby"</span><span class="p">,</span>
    <span class="p">[</span><span class="n">seller</span> <span class="k">for</span> <span class="n">seller</span> <span class="ow">in</span> <span class="n">df</span><span class="p">[</span><span class="s">"Seller"</span><span class="p">].</span><span class="nf">unique</span><span class="p">()],</span>
    <span class="p">[</span><span class="n">seller</span> <span class="k">for</span> <span class="n">seller</span> <span class="ow">in</span> <span class="n">df</span><span class="p">[</span><span class="s">"Seller"</span><span class="p">].</span><span class="nf">unique</span><span class="p">()],</span>
<span class="p">)</span>
<span class="n">filtered_df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s">"Seller"</span><span class="p">].</span><span class="nf">isin</span><span class="p">(</span><span class="n">sellers_to_filter</span><span class="p">)]</span>

<span class="c1"># Creates 2 columns for plots
</span><span class="n">col1</span><span class="p">,</span> <span class="n">col2</span> <span class="o">=</span> <span class="n">st</span><span class="p">.</span><span class="nf">columns</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Creates a bar chart
</span><span class="n">col1</span><span class="p">.</span><span class="nf">plotly_chart</span><span class="p">(</span><span class="n">px</span><span class="p">.</span><span class="nf">bar</span><span class="p">(</span><span class="n">filtered_df</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s">"Seller"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">"Sales"</span><span class="p">))</span>

<span class="c1"># Creates a pie chart
</span><span class="n">col2</span><span class="p">.</span><span class="nf">plotly_chart</span><span class="p">(</span><span class="n">px</span><span class="p">.</span><span class="nf">pie</span><span class="p">(</span><span class="n">filtered_df</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="s">"Sales"</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="s">"Seller"</span><span class="p">).</span><span class="nf">update_layout</span><span class="p">(</span><span class="n">showlegend</span><span class="o">=</span><span class="bp">False</span><span class="p">))</span>

<span class="c1"># Displays the DataFrame as a table
</span><span class="n">st</span><span class="p">.</span><span class="nf">table</span><span class="p">(</span><span class="n">filtered_df</span><span class="p">)</span>
</code></pre></div></div> <p>While we iterate through our perfect web app, we can use the following command to see the changes in real time:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>streamlit run app.py
</code></pre></div></div> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/streamlit-aws-03-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/streamlit-aws-03-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/streamlit-aws-03-1400.webp"/> <img src="/assets/img/streamlit-aws-03.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" alt="Our Streamlit application" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="containerizing-our-application">Containerizing our application</h2> <p>Now that our app is complete and we are happy with the results of this first iteration, we can start to think about how to deploy it. As we mentioned before, we could use <a href="https://streamlit.io/cloud">Streamlit Cloud</a> to host our application, but we will use AWS to have more control over our application and to be able to scale it in the future.</p> <p>Although we could deploy our application as a script directly in an EC2 instance, it’s advised to containerized it. This way, we can easily deploy it in any environment and we can also scale up to Kubernetes if needed, for example, or collaborate easier with other teams.</p> <p>To containerize our application, we will use Docker. We will <a href="https://docs.docker.com/get-docker/">install Docker</a> and create a <code class="language-plaintext highlighter-rouge">Dockerfile</code> with the following content:</p> <div class="language-dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">FROM</span><span class="s"> python:3.9</span>

<span class="k">EXPOSE</span><span class="s"> 8501</span>

<span class="k">WORKDIR</span><span class="s"> /app</span>

<span class="k">RUN </span>apt-get update <span class="o">&amp;&amp;</span> apt-get <span class="nb">install</span> <span class="nt">-y</span> <span class="se">\
</span>    build-essential <span class="se">\
</span>    software-properties-common <span class="se">\
</span>    git

<span class="k">COPY</span><span class="s"> ./requirements.txt /app/requirements.txt</span>

<span class="k">RUN </span>pip3 <span class="nb">install</span> <span class="nt">--upgrade</span> pip <span class="o">&amp;&amp;</span> pip <span class="nb">install</span> <span class="nt">-r</span> requirements.txt
</code></pre></div></div> <p>And we will also create a <code class="language-plaintext highlighter-rouge">docker-compose.yml</code> file with the following content to manage our image and prepare it to include further services if needed:</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">version</span><span class="pi">:</span> <span class="s2">"</span><span class="s">3.9"</span>
<span class="na">services</span><span class="pi">:</span>
  <span class="na">my-app</span><span class="pi">:</span>
    <span class="na">build</span><span class="pi">:</span>
      <span class="na">context</span><span class="pi">:</span> <span class="s">.</span>
      <span class="na">dockerfile</span><span class="pi">:</span> <span class="s">./Dockerfile</span>
    <span class="na">volumes</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">$HOME/.aws/config:/root/.aws/config</span>
      <span class="pi">-</span> <span class="s">$HOME/.aws/credentials:/root/.aws/credentials</span>
    <span class="na">command</span><span class="pi">:</span> <span class="s">streamlit run app.py --server.port=8501 --server.address=0.0.0.0</span>
    <span class="na">restart</span><span class="pi">:</span> <span class="s">always</span>
    <span class="na">ports</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">8501:8501</span>
</code></pre></div></div> <p>We can test our application in local using the following command on the root of our project:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker-compose up
</code></pre></div></div> <p>This should have opened a port 8501 in our localhost, where we can see our application running.</p> <h2 id="deploying-our-application">Deploying our application</h2> <h3 id="aws-configuration">AWS Configuration</h3> <p>In order to deploy our application in AWS, we first need to create an AWS account. If you don’t have one, you can create one <a href="https://aws.amazon.com/">here</a>. Please consider that we will only be using the free tier offered by Amazon (<a href="aws.amazon.com/free">more info</a>) to avoid extra costs with our examples. To create an alert in case you surpass the free tier usage go to Account Settings &gt; Budgets &gt; Create Budget and select the <code class="language-plaintext highlighter-rouge">Zero spend budget</code> template:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/streamlit-aws-04-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/streamlit-aws-04-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/streamlit-aws-04-1400.webp"/> <img src="/assets/img/streamlit-aws-04.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" alt="Create Budget Alert" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Then we can create a Group and a IAM user following the instructions <a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_create.html">here</a>, downloading the secret key and the access key. We will need these credentials to connect to our AWS account programmatically.</p> <p>Once you have your account, you can create an EC2 instance. We recommend using <strong>t2.micro</strong> with Amazon Linux for testing due to the low cost, but you can use a bigger instance if you want to scale up your application.</p> <p>On the Security Group configuration, we need to expose the port that’s going to be used for our application, which in this case will be the 8501. We also need to add a rule to allow SSH access to our instance, so we can connect to the instance later. On this step, it’s also important to create a key pair to authenticate our connection while connecting to the instance via SSH.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/streamlit-aws-05-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/streamlit-aws-05-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/streamlit-aws-05-1400.webp"/> <img src="/assets/img/streamlit-aws-05.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" alt="Security Group Configuration" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Once our EC2 is configured, feel free to run it using the AWS console so we can properly used it.</p> <h3 id="connecting-to-the-ec2-instance">Connecting to the EC2 instance</h3> <p>Now that our EC2 instance is running, we can connect to it using SSH. We can do that by running the following command:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ssh <span class="nt">-i</span> &lt;path-to-key-pair&gt; ec2-user@&lt;public-ip&gt;
</code></pre></div></div> <h3 id="installing-docker-in-ec2">Installing Docker in EC2</h3> <p>Once we are connected to our EC2 instance, we need to install Docker. We can do that by running the following commands:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>yum update <span class="nt">-y</span>
<span class="nb">sudo </span>amazon-linux-extras <span class="nb">install </span>docker
<span class="nb">sudo </span>service docker start
<span class="nb">sudo </span>usermod <span class="nt">-a</span> <span class="nt">-G</span> docker ec2-user
</code></pre></div></div> <p>And now we just need to download the code from our repository and build the image:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone &lt;git-repository-url&gt;
<span class="nb">cd</span> &lt;project-name&gt;
docker-compose up
</code></pre></div></div> <h3 id="adding-security-to-our-application">Adding security to our application</h3> <p>Streamlit can be integrated with SSOs such as <a href="https://www.okta.com">OKTA</a> and it’s the recommended option for production environments. However, it also offer different alternatives for use cases without SSO.</p> <p>The first alternative would be to use the <code class="language-plaintext highlighter-rouge">.streamlit/secrets.toml</code> file to store our credentials and later on check for the contents of this file in our application. An example of this integration, following the code in the <a href="https://docs.streamlit.io/knowledge-base/deploy/authentication-without-sso">Streamlit guidelines</a>, could be the following:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># .streamlit/secrets.toml</span>

<span class="o">[</span>passwords]
<span class="c"># Follow the rule: username = "password"</span>
alice_foo <span class="o">=</span> <span class="s2">"streamlit123"</span>
bob_bar <span class="o">=</span> <span class="s2">"mycrazypw"</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># app.py
</span><span class="kn">import</span> <span class="n">streamlit</span> <span class="k">as</span> <span class="n">st</span>

<span class="k">def</span> <span class="nf">check_password</span><span class="p">():</span>
    <span class="s">"""Returns `True` if the user had a correct password."""</span>

    <span class="k">def</span> <span class="nf">password_entered</span><span class="p">():</span>
        <span class="s">"""Checks whether a password entered by the user is correct."""</span>
        <span class="nf">if </span><span class="p">(</span>
            <span class="n">st</span><span class="p">.</span><span class="n">session_state</span><span class="p">[</span><span class="s">"username"</span><span class="p">]</span> <span class="ow">in</span> <span class="n">st</span><span class="p">.</span><span class="n">secrets</span><span class="p">[</span><span class="s">"passwords"</span><span class="p">]</span>
            <span class="ow">and</span> <span class="n">st</span><span class="p">.</span><span class="n">session_state</span><span class="p">[</span><span class="s">"password"</span><span class="p">]</span>
            <span class="o">==</span> <span class="n">st</span><span class="p">.</span><span class="n">secrets</span><span class="p">[</span><span class="s">"passwords"</span><span class="p">][</span><span class="n">st</span><span class="p">.</span><span class="n">session_state</span><span class="p">[</span><span class="s">"username"</span><span class="p">]]</span>
        <span class="p">):</span>
            <span class="n">st</span><span class="p">.</span><span class="n">session_state</span><span class="p">[</span><span class="s">"password_correct"</span><span class="p">]</span> <span class="o">=</span> <span class="bp">True</span>
            <span class="k">del</span> <span class="n">st</span><span class="p">.</span><span class="n">session_state</span><span class="p">[</span><span class="s">"password"</span><span class="p">]</span>  <span class="c1"># don't store username + password
</span>            <span class="k">del</span> <span class="n">st</span><span class="p">.</span><span class="n">session_state</span><span class="p">[</span><span class="s">"username"</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">st</span><span class="p">.</span><span class="n">session_state</span><span class="p">[</span><span class="s">"password_correct"</span><span class="p">]</span> <span class="o">=</span> <span class="bp">False</span>

    <span class="k">if</span> <span class="s">"password_correct"</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">st</span><span class="p">.</span><span class="n">session_state</span><span class="p">:</span>
        <span class="c1"># First run, show inputs for username + password.
</span>        <span class="n">st</span><span class="p">.</span><span class="nf">text_input</span><span class="p">(</span><span class="s">"Username"</span><span class="p">,</span> <span class="n">on_change</span><span class="o">=</span><span class="n">password_entered</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="s">"username"</span><span class="p">)</span>
        <span class="n">st</span><span class="p">.</span><span class="nf">text_input</span><span class="p">(</span>
            <span class="s">"Password"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="s">"password"</span><span class="p">,</span> <span class="n">on_change</span><span class="o">=</span><span class="n">password_entered</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="s">"password"</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="bp">False</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="n">st</span><span class="p">.</span><span class="n">session_state</span><span class="p">[</span><span class="s">"password_correct"</span><span class="p">]:</span>
        <span class="c1"># Password not correct, show input + error.
</span>        <span class="n">st</span><span class="p">.</span><span class="nf">text_input</span><span class="p">(</span><span class="s">"Username"</span><span class="p">,</span> <span class="n">on_change</span><span class="o">=</span><span class="n">password_entered</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="s">"username"</span><span class="p">)</span>
        <span class="n">st</span><span class="p">.</span><span class="nf">text_input</span><span class="p">(</span>
            <span class="s">"Password"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="s">"password"</span><span class="p">,</span> <span class="n">on_change</span><span class="o">=</span><span class="n">password_entered</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="s">"password"</span>
        <span class="p">)</span>
        <span class="n">st</span><span class="p">.</span><span class="nf">error</span><span class="p">(</span><span class="s">"😕 User not known or password incorrect"</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">False</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Password correct.
</span>        <span class="k">return</span> <span class="bp">True</span>

<span class="k">if</span> <span class="nf">check_password</span><span class="p">():</span>
    <span class="c1"># Here comes the main content of our application
</span></code></pre></div></div> <p>Even though we can use this approach, it’s not recommended to store our credentials in plain text. We can use <a href="https://aws.amazon.com/secrets-manager/">AWS Secrets Manager</a> to store our credentials and retrieve them in our application. This way, we can avoid storing our credentials in plain text and we can also use the same credentials for different applications. This approach is a little bit more complex, but I find it to be much more secure.</p> <p>After registering our secret in AWS Secrets Manager, we can retrieve and store it in our application session state by using code like the following:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">ast</span>
<span class="kn">import</span> <span class="n">boto3</span>
<span class="kn">import</span> <span class="n">botocore</span>

<span class="k">def</span> <span class="nf">state_password_dict</span><span class="p">(</span><span class="n">secret_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">region_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
    <span class="n">session</span> <span class="o">=</span> <span class="n">boto3</span><span class="p">.</span><span class="n">session</span><span class="p">.</span><span class="nc">Session</span><span class="p">()</span>
    <span class="n">client</span> <span class="o">=</span> <span class="n">session</span><span class="p">.</span><span class="nf">client</span><span class="p">(</span><span class="n">service_name</span><span class="o">=</span><span class="s">"secretsmanager"</span><span class="p">,</span> <span class="n">region_name</span><span class="o">=</span><span class="n">region_name</span><span class="p">)</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="n">get_secret_value_response</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="nf">get_secret_value</span><span class="p">(</span><span class="n">SecretId</span><span class="o">=</span><span class="n">secret_name</span><span class="p">)</span>
        <span class="n">secret_string</span> <span class="o">=</span> <span class="n">get_secret_value_response</span><span class="p">[</span><span class="s">"SecretString"</span><span class="p">]</span>
        <span class="n">secret_dict</span> <span class="o">=</span> <span class="n">ast</span><span class="p">.</span><span class="nf">literal_eval</span><span class="p">(</span><span class="n">secret_string</span><span class="p">)</span>
        <span class="n">st</span><span class="p">.</span><span class="n">session_state</span><span class="p">[</span><span class="s">"secret_dict"</span><span class="p">]</span> <span class="o">=</span> <span class="n">secret_dict</span>
    <span class="k">except</span> <span class="n">botocore</span><span class="p">.</span><span class="n">exceptions</span><span class="p">.</span><span class="n">ClientError</span><span class="p">:</span>
        <span class="c1"># if secret not found then create empty dict
</span>        <span class="n">st</span><span class="p">.</span><span class="n">session_state</span><span class="p">[</span><span class="s">"secret_dict"</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
</code></pre></div></div> <p>Later on, instead of checking the credentials given by the user against those in <code class="language-plaintext highlighter-rouge">st.secrets</code>, we can check them against the credentials stored in our session state by using <code class="language-plaintext highlighter-rouge">st.session_state["secret_dict"]</code> instead.</p> <blockquote> <p>Consider that in order to use this code we need to have in our EC2 instance the AWS config files and add them to our Docker image. You can find more information about this in the <a href="https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html">AWS documentation</a>. Go ahead and check our <code class="language-plaintext highlighter-rouge">docker-compose.yml</code> file to see how we did it.</p> </blockquote> <h2 id="final-remarks">Final remarks</h2> <p>Even though this guide was more focused on AWS, I hope it has illustrated how to easily deploy a ML application in the cloud. No matter which service or framework you use, at the end of the day the core concepts such as containerization and deployment are very similar regardless of our vendor choices.</p> <p>It’s true that Streamlit provide us a friendly framework to create our applications, but it’s also true that we need to be aware of the security and privacy issues that we are facing when deploying our code in the cloud. Some DevOps knowledge always come in handy when dealing with these issues, alongside with a good understanding of the cloud services that we are using.</p> <p>I hope that this guide has helped you to understand how to deploy your ML application in the cloud, and we hope to see you soon in our next post!</p>]]></content><author><name></name></author><category term="mlops"/><category term="aws"/><category term="python"/><summary type="html"><![CDATA[Deploy your Streamlit application into AWS and create easily a Data Science web app]]></summary></entry><entry><title type="html">Dealing with Imbalanced Datasets</title><link href="https://parsaomidi.github.io/blog/2023/dealing-with-imbalanced-datasets/" rel="alternate" type="text/html" title="Dealing with Imbalanced Datasets"/><published>2023-01-08T01:00:00+00:00</published><updated>2023-01-08T01:00:00+00:00</updated><id>https://parsaomidi.github.io/blog/2023/dealing-with-imbalanced-datasets</id><content type="html" xml:base="https://parsaomidi.github.io/blog/2023/dealing-with-imbalanced-datasets/"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/imbalanced-datasets-01-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/imbalanced-datasets-01-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/imbalanced-datasets-01-1400.webp"/> <img src="/assets/img/imbalanced-datasets-01.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <blockquote> <p><a href="https://github.com/josumsc/credit-card-fraud-detection">This repository</a> serves as an illustration of how this problem may appear in a real-world scenario and how to deal with it.</p> </blockquote> <p>Imagine the CFO of your organization comes to you complaining about how the recent uprise of e-commerce after the Covid outbreak has increased the risk of credit card fraud. As a Data Scientist, you start gathering data from the company’s financial records and start building a baseline model that can show the potential of this task. After a couple of days of work, you have a model that predicts fraudulent transactions with 99.9% precision. You run to the CFO to inform her/him of the good news but you get reprimanded as your model is deemed useless. What happened?</p> <p>The problem is that, as the dataset is highly imbalanced, you model is predicting that every one of the transactions is legit, so it’s unable to detect one single fraudulent transaction. This is a common problem in many industries, and it’s important to know how to deal with it.</p> <h2 id="what-is-an-imbalanced-dataset">What is an imbalanced dataset?</h2> <p>An imbalanced dataset is a dataset where the number of observations in one class is significantly higher than the number of observations in the other classes. In the example above, the number of fraudulent transactions is much lower than the number of legit transactions, with a proportion of 1 fraudulent transaction by at least 999 legit ones. This is a common problem in many industries, such as fraud detection, medical diagnosis, and mechanical engineering, where defective parts are much rare than correct ones.</p> <p>Imbalanced datasets cause problems in Machine Learning as models are focused on maximizing the accuracy of the ensemble of predictions. In the example above, the model will predict that every transaction is legit, and it will be right 99.9% of the time. This is a problem because the model will not able to detect any of the fraudulent transactions, proving itself useless for the task at hand.</p> <h2 id="how-to-deal-with-imbalanced-datasets">How to deal with imbalanced datasets</h2> <p>In the academic literature several methods are approached to deal with this particular problem. As with any other problem in Machine Learning, there is no such thing as a free lunch, and each method has its own pros and cons. In this post, I will cover the most common methods to deal with imbalanced datasets, and I will show how to implement them in Python.</p> <h3 id="dataset-resampling">Dataset resampling</h3> <p>One of the main ways to deal with imbalanced datasets is to directly resample them. The 2 main ways to perform this operation are Oversampling, which consists in adding more observations to the minority class, and Undersampling, which consists in removing observations from the majority class. In the example above, we could add more fraudulent transactions to the dataset, or we could remove legit transactions from the dataset.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/imbalanced-datasets-02-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/imbalanced-datasets-02-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/imbalanced-datasets-02-1400.webp"/> <img src="/assets/img/imbalanced-datasets-02.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Although we could use random approaches to perform both over and under sampling, to perform this operations we have more advanced methods such as <a href="https://arxiv.org/abs/1106.1813">SMOTE</a> or <a href="https://www.sciencedirect.com/science/article/pii/S0957417422005280">NearMiss</a>, both based on Nearest Neighbors models, for repectively over and under sampling. These methods are implemented in the <a href="https://imbalanced-learn.org/stable/">imbalanced-learn</a> Python library are easy enough to implement:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">imblearn.under_sampling</span> <span class="kn">import</span> <span class="n">NearMiss</span>
<span class="kn">from</span> <span class="n">imblearn.over_sampling</span> <span class="kn">import</span> <span class="n">SMOTE</span>

<span class="c1"># Change between the desired resampling method
</span><span class="n">resampler</span> <span class="o">=</span> <span class="nc">NearMiss</span><span class="p">()</span>
<span class="c1"># resampler = SMOTE()
</span>
<span class="n">X_resampled</span><span class="p">,</span> <span class="n">y_resampled</span> <span class="o">=</span> <span class="n">resampler</span><span class="p">.</span><span class="nf">fit_resample</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div></div> <p>This kind of transformers work quite fine out-of-the-box, but some parameters we may want to tune are the number of neighbors to consider when resampling, and the sampling proportions. In the example above, we are using the default parameters, but we could tune them to get better results.</p> <blockquote> <p>It’s extremely important not to resample the test partition of the dataset, as we want it to be as close as possible to the real world scenario, when the classes will keep their natural imbalance.</p> </blockquote> <h3 id="metric-selection">Metric Selection</h3> <p>Another way to deal with imbalanced datasets is to select a metric that is more focused on the minority class. In the example above, we could use the <a href="https://en.wikipedia.org/wiki/F1_score">F1 score</a> instead of the accuracy, as it is more focused on the minority class. This way, we will be able to detect the fraudulent transactions, even if we are not able to detect all of them.</p> <p>As with many other metrics, the F1 score is implemented in the <a href="https://scikit-learn.org/stable/">scikit-learn</a> library:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">f1_score</span>
<span class="kn">from</span> <span class="n">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="nf">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_informative</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                            <span class="n">n_redundant</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_repeated</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_classes</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                            <span class="n">n_clusters_per_class</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                            <span class="n">weights</span><span class="o">=</span><span class="p">[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.94</span><span class="p">],</span>
                            <span class="n">class_sep</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">clf</span> <span class="o">=</span> <span class="nc">LogisticRegression</span><span class="p">()</span>
<span class="n">clf</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"F1 obtained: </span><span class="si">{</span><span class="n">f1_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s">'macro'</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div> <p>If we are instead using PyTorch, we can use the <a href="https://torchmetrics.readthedocs.io/en/stable/classification/f1_score.html">F1 score</a> implemented in the <a href="https://torchmetrics.readthedocs.io/en/latest/">torchmetrics</a> library:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">torchmetrics</span> <span class="kn">import</span> <span class="n">F1Score</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">optim</span>

<span class="kn">from</span> <span class="n">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>

<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Simple neural network with 2 hidden layers
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="nf">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_informative</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                            <span class="n">n_redundant</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_repeated</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_classes</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                            <span class="n">n_clusters_per_class</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                            <span class="n">weights</span><span class="o">=</span><span class="p">[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.94</span><span class="p">],</span>
                            <span class="n">class_sep</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">from_numpy</span><span class="p">(</span><span class="n">X</span><span class="p">).</span><span class="nf">float</span><span class="p">()</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">from_numpy</span><span class="p">(</span><span class="n">y</span><span class="p">).</span><span class="nf">long</span><span class="p">()</span>

<span class="n">model</span> <span class="o">=</span> <span class="nc">Net</span><span class="p">()</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>

<span class="n">f1</span> <span class="o">=</span> <span class="nc">F1Score</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s">"multiclass"</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s">'macro'</span><span class="p">)</span>

<span class="k">with</span> <span class="nf">tqdm</span><span class="p">(</span><span class="n">total</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span> <span class="k">as</span> <span class="n">pbar</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="nf">zip</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)),</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">):</span>
            <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
            <span class="n">y_pred</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
            <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
        <span class="n">pbar</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">pbar</span><span class="p">.</span><span class="nf">set_description</span><span class="p">(</span><span class="sa">f</span><span class="s">"Loss: </span><span class="si">{</span><span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s"> - F1: </span><span class="si">{</span><span class="nf">f1</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"F1 obtained: </span><span class="si">{</span><span class="nf">f1</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div> <p>Some other useful metrics to consider here are the <a href="https://en.wikipedia.org/wiki/Precision_and_recall">Precision-Recall curve</a> and the <a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic">ROC curve</a>, so feel free to check them out while building and evaluating your models.</p> <h3 id="cost-sensitive-learning">Cost-sensitive learning</h3> <p>Another way to deal with imbalanced datasets is to use cost-sensitive learning. In this approach, we assign a cost to each class, and we use this cost to weight the loss function of the model. This way, the model will be more focused on minimizing the loss of the minority class, and it will be less focused on minimizing the loss of the majority class. This approach is implemented in the <a href="https://scikit-learn.org/stable/">scikit-learn</a> library with the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html">class_weight</a> parameter:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># The code would be the same as in the previous section but with the following change
</span><span class="n">clf</span> <span class="o">=</span> <span class="nc">LogisticRegression</span><span class="p">(</span><span class="n">class_weight</span><span class="o">=</span><span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span> <span class="mf">0.94</span><span class="p">})</span>
</code></pre></div></div> <p>A similar implementation can take place in PyTorch with the <a href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html">torch.nn.CrossEntropyLoss</a> class:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># The code would be the same as in the previous section but with the following change
</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">(</span><span class="n">weight</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.94</span><span class="p">]))</span>
</code></pre></div></div> <h3 id="do-nothing">Do nothing</h3> <p>Yes! You read that right. Sometimes, the best way to deal with imbalanced datasets is to do nothing. In some cases, the model will be able to detect the minority class even if it is not able to detect all of them. With neural networks this approach has gained some popularity recently due to their predictive power, so do not be afraid to try it out.</p> <h2 id="final-remarks">Final remarks</h2> <p>Imbalanced datasets are one of the most common problems on modern applications of Machine Learning. It’s difficult to stumble upon a dataset that is perfectly balanced in a production environment. In this article, we have seen some of the most common approaches to deal with imbalance, and we have seen how to implement them in Python. The key takeaway from this article is that there is no one-size-fits-all solution to this problem. You will have to validate several approaches with the test set against imbalance robust metrics such as F1 or AUC.</p> <p>I hope that you have found this article useful, and that you will be able to use these techniques in your future projects. See you soon!</p>]]></content><author><name></name></author><category term="data preparation"/><category term="pytorch"/><category term="python"/><summary type="html"><![CDATA[Imbalanced datasets can lead to 99% precision and 0% predictive power, how could we then fit our models?]]></summary></entry><entry><title type="html">Explicit and Implicit Collaborative Filtering</title><link href="https://parsaomidi.github.io/blog/2022/collaborative-filtering/" rel="alternate" type="text/html" title="Explicit and Implicit Collaborative Filtering"/><published>2022-07-30T01:00:00+00:00</published><updated>2022-07-30T01:00:00+00:00</updated><id>https://parsaomidi.github.io/blog/2022/collaborative-filtering</id><content type="html" xml:base="https://parsaomidi.github.io/blog/2022/collaborative-filtering/"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/collaborative-filtering-01-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/collaborative-filtering-01-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/collaborative-filtering-01-1400.webp"/> <img src="/assets/img/collaborative-filtering-01.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>There are several situations when we may want to recommend a product or a piece of content to a user in order to increase our revenue, the engagement of the user with our site or the value perceived by the user while experiencing our site. We cannot recommend items that the user has already left a review on, as he or she has already seen the product and is well-aware of its features. Therefore, we must adventure and try to estimate which items of our catalog are the ones that the user might be interested in, without having an explicit signal for that particular item/user combination.</p> <p>This is where Collaborative Filtering (CF) comes in. Collaborative Filtering is a technique that allows us to estimate the interest of a user to a set of items. The idea is that we can use the reviews of other users to estimate the interest of a different user on the items that those first users reviewed. It is similar to say:</p> <blockquote> <p>“Person A liked products X, Y and Z. Person B liked products X and Z. Therefore, B might like Y with a reasonable confidence.”.</p> </blockquote> <p>This interesting and intuitive process is done by assigning $K$ latent features for each item, and then infer for both the users and the items each of these features so we can see how the user and the item are related. These latent factors can be imagined as the implicit features that our items have, such as the “recentness” of an item in the market, how eco-friendly it is or how related is to a particular topic, such as culture or health, but in the end they are just vectors that are optimized to minimize a cost function, similarly as it happens with <em>word embeddings</em> in NLP tasks.</p> <p>In order to make these predictions and approximate the users preferences successfully we must check the data available and thus the restrictions of the models we may develop to help us with the task at hand. Depending on the data we have, there are two main branches of collaborative filtering models:</p> <ol> <li><strong>Explicit Collaborative Filtering</strong>, which are the most common models, and the ones that are ancient and proven by the research community. To apply them we need an abundance of explicit scores on the user/item combinations, such a rating from 1 to 5 stars or a good/bad review.</li> <li><strong>Implicit Collaborative Filtering</strong>, which are models trained without any explicit score at a user/item level. To apply them we use the signals left by the user on our sites, such as the item page views, the add-to-cart events or the purchases of the user.</li> </ol> <h2 id="explicit-collaborative-filtering">Explicit Collaborative Filtering</h2> <blockquote> <p>For an example of Explicit Collaborative Filtering please check this repository: <a href="https://github.com/josumsc/movielens-recommender">MovieLens Recommender</a></p> </blockquote> <p>As we mentioned before, the explicit collaborative filtering models are the ones that try to predict a known variable, which could be a continuous value as a rating from 1 to 5 stars or a categorical value as a good/bad review. Thus, we face a supervised learning problem, given a matrix X of user/item combinations and a vector y of explicit scores, and depending on the form of the scores we will be using a regressor or a classifier. Now that we know the kind of problem we are facing, it is simpler to come up with good solutions to solve it, so let’s explore a few.</p> <h3 id="classic-collaborative-filtering">Classic Collaborative Filtering</h3> <p>A few years back, when Deep Learning was not still as popular as it is and computational resources were scarce, we had to use a very simple model to predict the ratings of our users. Although, this primordial model was not bad at all, and it served as foundation to the vast majority of the newer models. This model was simply called <strong>Collaborative Filtering</strong> and it was based on the random assignment of $K$ latent factors for the matrices of users and items, and then use those matrices to infer the ratings of the users to the items by multiplying them. After that, we could compute our loss depending on the kind of label we have (either <em>MSE</em> for regression or <em>crossentropy</em> for classification, for example) and if the score was off then we could use an optimizer such as <em>SGD</em> to adjust the latent factors to the correct value. Then, we would compute again the ratings of the users to the items, finishing our training cycle.</p> <blockquote> <p>If we want a more in depth view of classic collaborative filtering, <a href="https://www.coursera.org/learn/machine-learning-course/lecture/2WoBV/collaborative-filtering">this video by Andrew Ng</a> is an invaluable resource.</p> </blockquote> <p>We can apply this method using simple <code class="language-plaintext highlighter-rouge">NumPy</code>, but as it is based on linear algebra and we might want to take advantage of our GPUs let’s see an example using <code class="language-plaintext highlighter-rouge">PyTorch</code>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Recommender</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">n_users</span><span class="p">,</span> <span class="n">n_movies</span><span class="p">,</span> <span class="n">emb_size</span><span class="p">,</span> <span class="n">y_range</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">Recommender</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">user_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">n_users</span><span class="p">,</span> <span class="n">emb_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">user_bias</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">n_users</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">movie_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">n_movies</span><span class="p">,</span> <span class="n">emb_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">movie_bias</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">n_movies</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">y_range</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([[</span><span class="n">y_range</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">users</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">user_embedding</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span>
        <span class="n">movies</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">movie_embedding</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
        <span class="n">result</span> <span class="o">=</span> <span class="p">(</span><span class="n">users</span> <span class="o">*</span> <span class="n">movies</span><span class="p">).</span><span class="nf">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">result</span> <span class="o">+=</span> <span class="n">result</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">user_bias</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">movie_bias</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">result</span><span class="p">.</span><span class="nf">clip</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="n">y_range</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">max</span><span class="o">=</span><span class="n">y_range</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</code></pre></div></div> <p>As we can see, we instantiate both the weights and biases for each user and item and then simply multiply their weight matrices together. We also add the bias to the result of the multiplication, and we clip the result to the range of the ratings we have in order to make our network converge faster. We could instantiate and train the model using this snippet of code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">loader</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">):</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Starting epoch </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s">..."</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">loader</span><span class="p">:</span>
            <span class="n">y_hat</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
            <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Loss obtained: </span><span class="si">{</span><span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>


<span class="n">n_users</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">userId</span><span class="p">.</span><span class="nf">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">n_movies</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">movieId</span><span class="p">.</span><span class="nf">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">emb_size</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">y_range</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="nc">Recommender</span><span class="p">(</span><span class="n">n_users</span><span class="p">,</span> <span class="n">n_movies</span><span class="p">,</span> <span class="n">emb_size</span><span class="p">,</span> <span class="n">y_range</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">MSELoss</span><span class="p">()</span>

<span class="nf">train_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">)</span>
</code></pre></div></div> <p>Despite its simplicity, this model should be able to predict the ratings of our users with a decent error if we tune the number of latent factors and the learning rate accordingly, although in the end it’s just a linear model and it has its drawbacks and limitations, so let’s see how we can build upon it.</p> <h3 id="deep-collaborative-filtering">Deep Collaborative Filtering</h3> <p>Breakthroughs as the adoption of GPUs by the deep learning community, the availability of immense quantities of data and new parameter initialization techniques or activation functions have awaken a new wave of interest in the field of deep learning and neural networks. This also apply to the field of recommender systems, as thousands of websites can make use of better recommendations for their users.</p> <p>Enter the <strong>feed-forward collaborative filtering models</strong>, which build upon the latent factors of the classic collaborative filtering models by adding several layers on top of them. These layers allow our models to find deeper and more complicated relationships between the users and the items, and thus, to predict the ratings of the users to the items with a better accuracy. It is important to mind that these models are nothing but an application of the ideas previously seen, which are sometimes more than enough to provide good recommendations, and that they come with a higher computational cost, so it may be wise to test first a simpler model and then iterate upon it.</p> <p>Continuing our PyTorch code, we now will see how to build this kind of models:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">FFRecommender</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">n_users</span><span class="p">,</span> <span class="n">n_movies</span><span class="p">,</span> <span class="n">emb_size</span><span class="p">,</span> <span class="n">n_units</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">y_range</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">FFRecommender</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">user_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">n_users</span><span class="p">,</span> <span class="n">emb_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">movie_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">n_movies</span><span class="p">,</span> <span class="n">emb_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">emb_size</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">n_units</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_units</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">y_range</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([[</span><span class="n">y_range</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">users</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">user_embedding</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span>
        <span class="n">movies</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">movie_embedding</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
        <span class="n">h1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">linear1</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">users</span><span class="p">,</span> <span class="n">movies</span><span class="p">],</span> <span class="mi">1</span><span class="p">)))</span>
        <span class="n">h1</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">h1</span><span class="p">)</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">linear2</span><span class="p">(</span><span class="n">h1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">result</span><span class="p">.</span><span class="nf">clip</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="n">y_range</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">max</span><span class="o">=</span><span class="n">y_range</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</code></pre></div></div> <p>The additions to the code come as linear layers. We also added a dropout layer as regularization and drop the matrix multiplication, as PyTorch’s <code class="language-plaintext highlighter-rouge">nn.Linear</code> does that for us. In this occasion, the code to instantiate and train the model is very similar to the one we used for the classic collaborative filtering model:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">n_units</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">dropout</span> <span class="o">=</span> <span class="mf">0.2</span>

<span class="n">ff_model</span> <span class="o">=</span> <span class="nc">FFRecommender</span><span class="p">(</span><span class="n">n_users</span><span class="p">,</span> <span class="n">n_movies</span><span class="p">,</span> <span class="n">emb_size</span><span class="p">,</span> <span class="n">n_units</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">y_range</span><span class="p">)</span>
<span class="n">ff_model</span> <span class="o">=</span> <span class="n">ff_model</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">ff_model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">MSELoss</span><span class="p">()</span>

<span class="nf">train_model</span><span class="p">(</span><span class="n">ff_model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">)</span>
</code></pre></div></div> <p>These models are expected to reach a better score than the previous ones due to their higher complexity, although it comes at a cost in form of more hyperparameters to tune and higher computational cost.</p> <h3 id="sequential-collaborative-filtering">Sequential Collaborative Filtering</h3> <p>Continuing our journey down the rabbit hole of neural networks, we can now build a <strong>sequential collaborative filtering model</strong>. This model is a combination of the feed-forward and the recurrent models, which allow us to think of user preferences not as a fixed vector at a user level but more as a sequence based tensor that depends on the particularities of the items reviewed and their order. The idea behind this model is to capture the effect of recent reviews of users on the next item to be scored as, for instance, a user may decrease the score of a romance movie if he or she has only been watching this genre of movies lately and grew tired of them.</p> <p>On most of the occasions, our data will be in a tabular form, so we may need first to convert them to sequences to pass through our model:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sequences</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">userId</span> <span class="ow">in</span> <span class="n">df_train</span><span class="p">.</span><span class="n">userId</span><span class="p">.</span><span class="nf">unique</span><span class="p">():</span>
    <span class="n">df_filtered</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s">'userId'</span><span class="p">]</span> <span class="o">==</span> <span class="n">userId</span><span class="p">].</span><span class="nf">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s">'timestamp'</span><span class="p">)</span>
    <span class="n">user_sequence</span> <span class="o">=</span> <span class="p">(</span><span class="n">df_filtered</span><span class="p">[</span><span class="s">'movieId'</span><span class="p">].</span><span class="n">values</span><span class="p">[:</span><span class="mi">10</span><span class="p">],</span> <span class="n">df_filtered</span><span class="p">[</span><span class="s">'rating'</span><span class="p">].</span><span class="n">values</span><span class="p">[:</span><span class="mi">10</span><span class="p">])</span>
    <span class="n">sequences</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">user_sequence</span><span class="p">)</span>

<span class="n">cutoff_index</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">sequences</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.8</span><span class="p">)</span>
<span class="n">sequences_train</span><span class="p">,</span> <span class="n">sequences_test</span> <span class="o">=</span> <span class="n">sequences</span><span class="p">[:</span><span class="n">cutoff_index</span><span class="p">],</span> <span class="n">sequences</span><span class="p">[</span><span class="n">cutoff_index</span><span class="p">:]</span>
</code></pre></div></div> <p>Now that we created the sequences to serve to our model, we may use Recurrent Neural Networks (RNNs) to implement this solution. The most common RNNs are LSTMs and GRUs, and due to its higher complexity and popularity we will be using LSTMs here:</p> <blockquote> <p>For a more in depth explanation of RNNs, please refer to <a href="https://www.youtube.com/watch?v=SEnXr6v2ifU">this video by MIT</a>.</p> </blockquote> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SeqRecommender</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">num_items</span><span class="p">,</span> <span class="n">num_output</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span>
        <span class="n">self</span><span class="p">.</span><span class="n">item_embeddings</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">num_items</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">lstm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LSTM</span><span class="p">(</span><span class="n">embedding_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">num_output</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">hidden</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">init_hidden</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">init_hidden</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="c1"># initialize both hidden layers
</span>        <span class="nf">return </span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="nc">Variable</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">hidden_dim</span><span class="p">)),</span>
                <span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="nc">Variable</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">hidden_dim</span><span class="p">)))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">sequence</span><span class="p">):</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">item_embeddings</span><span class="p">(</span><span class="n">sequence</span><span class="p">)</span>
        <span class="n">output</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">hidden</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">lstm</span><span class="p">(</span><span class="n">embeddings</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">sequence</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span>
                                        <span class="n">self</span><span class="p">.</span><span class="n">hidden</span><span class="p">)</span>
        <span class="n">rating_scores</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">linear</span><span class="p">(</span><span class="n">output</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">sequence</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">rating_scores</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">sequence</span><span class="p">):</span>
        <span class="n">rating_scores</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">forward</span><span class="p">(</span><span class="n">sequence</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">rating_scores</span>
</code></pre></div></div> <p>Regarding the training of our model, we may again see that the code is quite similar:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">n_units_lstm</span> <span class="o">=</span> <span class="mi">30</span>

<span class="n">seq_model</span> <span class="o">=</span> <span class="nc">SeqRecommender</span><span class="p">(</span><span class="n">emb_size</span><span class="p">,</span> <span class="n">n_units_lstm</span><span class="p">,</span> <span class="n">n_movies</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">MSELoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">seq_model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="p">)</span>

<span class="n">seq_model</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">25</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">sequence</span><span class="p">,</span> <span class="n">target_ratings</span> <span class="ow">in</span> <span class="n">sequences_train</span><span class="p">:</span>
        <span class="n">seq_model</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
        <span class="n">seq_model</span><span class="p">.</span><span class="n">hidden</span> <span class="o">=</span> <span class="n">seq_model</span><span class="p">.</span><span class="nf">init_hidden</span><span class="p">()</span>
        <span class="n">sequence_var</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="nc">Variable</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nc">LongTensor</span><span class="p">(</span><span class="n">sequence</span><span class="p">.</span><span class="nf">astype</span><span class="p">(</span><span class="s">'int64'</span><span class="p">)))</span>
        <span class="n">ratings_scores</span> <span class="o">=</span> <span class="nf">seq_model</span><span class="p">(</span><span class="n">sequence_var</span><span class="p">)</span>
        <span class="n">target_ratings_var</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="nc">Variable</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nc">FloatTensor</span><span class="p">(</span><span class="n">target_ratings</span><span class="p">.</span><span class="nf">astype</span><span class="p">(</span><span class="s">'float32'</span><span class="p">)))</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">ratings_scores</span><span class="p">,</span> <span class="n">target_ratings_var</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">(</span><span class="n">retain_graph</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Loss obtained on epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div> <p>In case we want to make our model more complex, we may want to add a second layer of LSTM to the model, or a second linear layer after the first one. Nevertheless, I still recommend using first a simpler model, evaluate the results and build add more layers to it if necessary.</p> <h2 id="implicit-collaborative-filtering">Implicit Collaborative Filtering</h2> <blockquote> <p>We can see the code of a practical example on this kind of tasks here: <a href="https://github.com/josumsc/retrocket-implicit-recommender">Retail Rocket Recommender</a></p> </blockquote> <p>The methods for <em>Explicit Collaborative Filtering</em> are straight-forward, effective and widely used by industry peers. Nevertheless, they require many data points of users explicitly informing us of how aligned a particular item is to their interests, which cannot always be obtained easily due to restrictions in our website or legal issues, for instance. We mentioned in the beginning of this article that there are methods to deal with training collaborative filtering models on implicit datasets, and we will see the most renowned one: <strong>Alternating Least Squares</strong>.</p> <p>The Alternating Least Squares (ALS) method was <a href="http://yifanhu.net/PUB/cf.pdf">first reviewed by Yifan Hu et al</a> and describes the use of a <em>confidence</em> matrix composed by the aggregated signal of the events a particular user generated while interacting with a certain product to infer the <em>preference</em> matrix, which is the binary version of the previous matrix and determines if a user has a preference for a particular product or not. Once we have the preference matrix, we compare it with the dot product of the latent factors of our users and items and weight that score by multiplying the confidence matrix, so the combinations with more signal are the ones that add more to the cost. Lastly, we will also be including a regularization parameter $\lambda$ which will help us avoid overfitting using a similar technique to L2 regularization. In this kind of models we have to avoid overfitting as much as possible as it would lead our estimators to only repeat the combinations seen previously in the training data.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/collaborative-filtering-02-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/collaborative-filtering-02-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/collaborative-filtering-02-1400.webp"/> <img src="/assets/img/collaborative-filtering-02.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Using a business case of an e-commerce trying to give their users the most adequate recommendations, we could apply ALS by first computing our <em>confidence</em> matrix, weighting the different signals of the user/item interactions to a single value. We will use for this case the following signals:</p> <ol> <li><strong>Product Page views</strong>, which will score as 1 in our confidence matrix.</li> <li><strong>Add-to-cart events</strong>, which will score as 5.</li> <li><strong>Transaction events</strong>, which will score as 25.</li> </ol> <p>With the difference on scores for each signal we expect to prioritize the strongest events above more frequent ones which may be deceiving, such as the page views. We can use Python to compute this confidence matrix easily given those weights:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span><span class="p">[</span><span class="s">'event_scores'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">'event'</span><span class="p">].</span><span class="nf">apply</span><span class="p">(</span>
    <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">x</span> <span class="o">==</span> <span class="s">'view'</span> <span class="k">else</span> <span class="mi">5</span> <span class="k">if</span> <span class="n">x</span> <span class="o">==</span> <span class="s">'addtocart'</span> <span class="k">else</span> <span class="mi">25</span> <span class="k">if</span> <span class="n">x</span> <span class="o">==</span> <span class="s">'transaction'</span> <span class="k">else</span> <span class="mi">0</span>
<span class="p">)</span>
</code></pre></div></div> <p>Later on, we make this matrix sparse, as it’s mostly composed by 0s and this conversion will speed up training and save us a lot of memory space that in cases with thousands of products and clients might be critical:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span><span class="p">[</span><span class="s">'visitorid'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">'visitorid'</span><span class="p">].</span><span class="nf">astype</span><span class="p">(</span><span class="s">"category"</span><span class="p">).</span><span class="n">cat</span><span class="p">.</span><span class="nf">as_ordered</span><span class="p">()</span>
<span class="n">df</span><span class="p">[</span><span class="s">'itemid'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">'itemid'</span><span class="p">].</span><span class="nf">astype</span><span class="p">(</span><span class="s">"category"</span><span class="p">).</span><span class="n">cat</span><span class="p">.</span><span class="nf">as_ordered</span><span class="p">()</span>

<span class="n">sparse_item_user</span> <span class="o">=</span> <span class="n">sparse</span><span class="p">.</span><span class="nf">csr_matrix</span><span class="p">((</span><span class="n">df</span><span class="p">[</span><span class="s">'event_scores'</span><span class="p">].</span><span class="nf">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">),</span> <span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'itemid'</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s">'visitorid'</span><span class="p">])))</span>
<span class="n">sparse_user_item</span> <span class="o">=</span> <span class="n">sparse</span><span class="p">.</span><span class="nf">csr_matrix</span><span class="p">((</span><span class="n">df</span><span class="p">[</span><span class="s">'event_scores'</span><span class="p">].</span><span class="nf">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">),</span> <span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'visitorid'</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s">'itemid'</span><span class="p">])))</span>
</code></pre></div></div> <p>Once the matrix is built and on a sparse form, we may simply use the ALS implementation of the library <code class="language-plaintext highlighter-rouge">implicit</code> to infer the latent factors of our items and users.</p> <blockquote> <p>P.S: Although ALS is the most common method for implicit recommendations, the <code class="language-plaintext highlighter-rouge">implicit</code> library has implementations for the rest of algorithms available for this task. If you are curious about them please go ahead and visit <a href="https://implicit.readthedocs.io/en/latest/">its documentation</a>.</p> </blockquote> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">latent_factors</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">regularization</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">n_iter</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mi">40</span>

<span class="n">conf_matrix</span> <span class="o">=</span> <span class="p">(</span><span class="n">sparse_item_user</span> <span class="o">*</span> <span class="n">alpha</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="s">'double'</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">implicit</span><span class="p">.</span><span class="n">als</span><span class="p">.</span><span class="nc">AlternatingLeastSquares</span><span class="p">(</span>
    <span class="n">factors</span><span class="o">=</span><span class="n">latent_factors</span><span class="p">,</span>
    <span class="n">regularization</span><span class="o">=</span><span class="n">regularization</span><span class="p">,</span>
    <span class="n">iterations</span><span class="o">=</span><span class="n">n_iter</span>
<span class="p">)</span>

<span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">conf_matrix</span><span class="p">)</span>
</code></pre></div></div> <p><em>Et voilà!</em>, our model is trained and ready to make suggestions for our users. Not only that, but we can also see which products are most similar among them, so we can infer substitute items in case of stock outs or to suggest products that solve the same needs but provide us a higher margin. The code for both this use cases would be the following:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">recommend_item_to_user</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">visitorid</span><span class="p">,</span> <span class="n">sparse_item_user</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">recommended</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">recommend</span><span class="p">(</span><span class="n">visitorid</span><span class="p">,</span> <span class="n">sparse_item_user</span><span class="p">[</span><span class="n">visitorid</span><span class="p">],</span> <span class="n">n</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">recommended</span>


<span class="k">def</span> <span class="nf">similar_items_to_item</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">itemid</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">similar</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">similar_items</span><span class="p">(</span><span class="n">itemid</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">similar</span>
</code></pre></div></div> <p>And we can call these functions as we would do with any Python module:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Choose an userid
</span><span class="n">userid</span> <span class="o">=</span> <span class="mi">97154</span>
<span class="n">recommended_items</span> <span class="o">=</span> <span class="nf">recommend_item_to_user</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">userid</span><span class="p">,</span> <span class="n">sparse_item_user</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Recommended items for user </span><span class="si">{</span><span class="n">userid</span><span class="si">}</span><span class="s">:</span><span class="se">\n</span><span class="si">{</span><span class="n">recommended_items</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

<span class="c1"># Choose an itemid
</span><span class="n">itemid</span> <span class="o">=</span> <span class="mi">350566</span>
<span class="n">similar_items</span> <span class="o">=</span> <span class="nf">similar_items_to_item</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">itemid</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Similar items to </span><span class="si">{</span><span class="n">itemid</span><span class="si">}</span><span class="s">:</span><span class="se">\n</span><span class="si">{</span><span class="n">similar_items</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div> <h2 id="conclusions">Conclusions</h2> <p>In this post we were able to see in detail the main mechanism behind the recommendations we often see on sites as Netflix, Amazon or Alibaba and apply different techniques to increase their capabilities or apply regularization layers as Dropout on them. We also saw how to deal with not having a good enough explicit dataset by using the signals left by our users in our site such as the purchases or the page views to infer their preferences.</p> <p>Hope this was useful and in case you have any doubt about the concepts explained on this article please do not hesitate to connect with me to further discussions. Thank you and keep on learning!</p>]]></content><author><name></name></author><category term="python"/><category term="deep learning"/><category term="pytorch"/><category term="recommender system"/><category term="e-commerce"/><summary type="html"><![CDATA[How can we get to the best recommendation possible when little to no reviews are available?]]></summary></entry><entry><title type="html">Transfer Learning: Standing on the Shoulders of Giants</title><link href="https://parsaomidi.github.io/blog/2022/transfer-learning/" rel="alternate" type="text/html" title="Transfer Learning: Standing on the Shoulders of Giants"/><published>2022-07-22T01:00:00+00:00</published><updated>2022-07-22T01:00:00+00:00</updated><id>https://parsaomidi.github.io/blog/2022/transfer-learning</id><content type="html" xml:base="https://parsaomidi.github.io/blog/2022/transfer-learning/"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/transfer-learning-01-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/transfer-learning-01-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/transfer-learning-01-1400.webp"/> <img src="/assets/img/transfer-learning-01.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <blockquote> <p><a href="https://github.com/josumsc/dogs-vs-cats">This repository</a> holds a quick demo of a practical use of the concepts in this article.</p> </blockquote> <p>While trying to solve a machine learning proble, most of us pass through an iterative process in which, after the data collection and analysis tasks, we try different preprocessing and different models to optimize a certain metric, either the crossentropy loss for a classification model or the Huber loss in a regression model, for example.</p> <p>On that process, a <a href="https://towardsdatascience.com/overfitting-vs-underfitting-a-complete-example-d05dd7e19765">cycle of underfitting and overfitting</a> is often repeated. In this cycle the first phase is often characterized by our models not being sufficiently complex to capture the different patterns of the data, followed by a phase when we make them too complex and we make them learn specific features of the training set instead of general features and characteristics that could be used in a different dataset.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/transfer-learning-02-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/transfer-learning-02-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/transfer-learning-02-1400.webp"/> <img src="/assets/img/transfer-learning-02.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Regularization techniques such as <a href="https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/">Dropout</a> or <a href="https://neptune.ai/blog/fighting-overfitting-with-l1-or-l2-regularization">L2 regularization</a> can help us solve the problem of overfitting, as they penalize the complexity of the model. The implementation of these methods is often recommended above the restriction of units per layer or the number of layers, as they are prepared to let those important parameters to compensate for the loss added to the model.</p> <p>Nevertheless, no matter how many regularization we use we eventually end up facing a barrier that we cannot overcome: The scarce volume of data we have. Even if we try to help our models, they only learn what is present in the training set, and if the training set is small or its diversity is not enough our models will eventually fail. In the end, they need to reduce a certain metric as much as possible, and they will adapt to the data whenever possible, so either we make them adapt too much (overfitting) to reduce the training loss, or too little (underfitting) to reduce the discrepancy between the training and validation losses.</p> <p>To help with this problem, we can use <strong>data augmentation</strong>. For example, if we were to use <code class="language-plaintext highlighter-rouge">Keras</code> to train a model, we could use the <code class="language-plaintext highlighter-rouge">ImageDataGenerator</code> to create synthetic data from the training set, and use it to train our model.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">tensorflow.keras.preprocessing.image</span> <span class="kn">import</span> <span class="n">ImageDataGenerator</span>

<span class="n">datagen</span> <span class="o">=</span> <span class="nc">ImageDataGenerator</span><span class="p">(</span>
      <span class="n">rotation_range</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
      <span class="n">width_shift_range</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
      <span class="n">height_shift_range</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
      <span class="n">shear_range</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
      <span class="n">zoom_range</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
      <span class="n">horizontal_flip</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
      <span class="n">fill_mode</span><span class="o">=</span><span class="s">'nearest'</span>
<span class="p">)</span>
</code></pre></div></div> <p>Here we can see how using parameters as <code class="language-plaintext highlighter-rouge">horizontal_flip</code> we could create a new image from another one by rotating it over its Y axis, which would imply giving our model an example more for capturing those generic patterns that we commented before.</p> <p>Although, this type of techniques end up using our training set as well, so if this is very poor, we will not be able to solve our problem. In this case, wouldn’t it be great to import a model trained by another person in a better dataset, with a better architecture? Enter Transfer Learning.</p> <h2 id="transfer-learning">Transfer Learning</h2> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/transfer-learning-03-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/transfer-learning-03-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/transfer-learning-03-1400.webp"/> <img src="/assets/img/transfer-learning-03.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>By using public available model repositories such as <a href="https://www.tensorflow.org/hub?hl=es-419">Tensorflow Hub</a> we can access pre-trained models and download them for use in our notebooks. In addition, well known models as GloVe or VGG16 have functions inside their own modules in Keras that we can access in order to load various implementations of those models.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">tensorflow.keras.applications</span> <span class="kn">import</span> <span class="n">VGG16</span>

<span class="n">conv_base</span> <span class="o">=</span> <span class="nc">VGG16</span><span class="p">(</span>
    <span class="n">weights</span><span class="o">=</span><span class="s">'imagenet'</span><span class="p">,</span>
    <span class="n">include_top</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
    <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># Shows the structure of the convnet
</span><span class="n">conv_base</span><span class="p">.</span><span class="nf">summary</span><span class="p">()</span>

<span class="c1"># Freezing the layers so they don't get modified at training
</span><span class="n">conv_base</span><span class="p">.</span><span class="n">trainable</span> <span class="o">=</span> <span class="bp">False</span>
</code></pre></div></div> <p>This step load the first layers (often called the <em>body</em> of the network) of the VGG16 model, as the last layers (those usually called the <em>head</em>) are specific for each task, as they are the ones in charge of classifying the input images in the different classes. In this case, adding a classifier over the pretrained model would be easy, and would allow us to train our model together using the same methods we would use with a model conceived completely by us:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="n">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Flatten</span><span class="p">,</span> <span class="n">Dropout</span>

<span class="n">model</span> <span class="o">=</span> <span class="nc">Sequential</span><span class="p">([</span>
    <span class="n">conv_base</span><span class="p">,</span>
    <span class="nc">Flatten</span><span class="p">(),</span>
    <span class="nc">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="nf">l2</span><span class="p">(</span><span class="mf">0.001</span><span class="p">)),</span>
    <span class="nc">Dropout</span><span class="p">(</span><span class="mf">0.3</span><span class="p">),</span>
    <span class="nc">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="nf">l2</span><span class="p">(</span><span class="mf">0.001</span><span class="p">)),</span>
    <span class="nc">Dropout</span><span class="p">(</span><span class="mf">0.3</span><span class="p">),</span>
    <span class="nc">Dense</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'softmax'</span><span class="p">),</span>
<span class="p">])</span>

<span class="n">model</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="s">'rmsprop'</span><span class="p">,</span>
    <span class="n">loss</span><span class="o">=</span><span class="s">'categorical_crossentropy'</span><span class="p">,</span>
    <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">]</span>
<span class="p">)</span>

<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span>
    <span class="n">train_generator</span><span class="p">,</span>
    <span class="n">steps_per_epoch</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">validation_data</span><span class="o">=</span><span class="n">validation_generator</span><span class="p">,</span>
    <span class="n">validation_steps</span><span class="o">=</span><span class="mi">250</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div></div> <h2 id="fine-tuning">Fine Tuning</h2> <p>The performance of our model at this point would have increased by a large margin, as using a model which already knew the general patterns of the images presented would have saved us tons of time and effort. We were able to tune the model by simply adding a <em>head block</em> on top of the pretrained model, but what happens if we start having more and more data or if the pretrained model does not fit our needs? In those cases we could apply Fine Tuning to further adjust the pretrained model so it can perform better in our particular task.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/transfer-learning-04-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/transfer-learning-04-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/transfer-learning-04-1400.webp"/> <img src="/assets/img/transfer-learning-04.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Fine Tuning is based on the idea of unfreeze the last layers of the model (those which encapsulate more specific patterns and features) and then train our model in our specific problem, so their weights get more adequate to the task at hand. We should consider that this is a risky practice, as if we modify the weights without being sure that the quality of our training set is adequate, we could end up making our model worse or even falling under the <a href="https://towardsdatascience.com/forgetting-in-deep-learning-4672e8843a7f">Catastrophic Forgetting problem</a>. Nevertheless, it is worth a try and very advisable when we know at which extend to use it. As a rule of thumb, avoid modifying the first layers of a pretrained model, and use a small learning rate while fine tuning.</p> <p>Keras offers a very friendly interface to do this in the last layers, as we can see in this snippet inspired in the book <a href="https://www.manning.com/books/deep-learning-with-python">Deep Learning with Python from François Chollet</a>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">conv_base</span> <span class="o">=</span> <span class="nc">VGG16</span><span class="p">(</span>
    <span class="n">weights</span><span class="o">=</span><span class="s">'imagenet'</span><span class="p">,</span>
    <span class="n">include_top</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
    <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="p">)</span>

<span class="n">set_trainable</span> <span class="o">=</span> <span class="bp">False</span>
<span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">conv_base</span><span class="p">.</span><span class="n">layers</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">layer</span><span class="p">.</span><span class="n">name</span> <span class="o">==</span> <span class="s">'block5_conv1'</span><span class="p">:</span>
        <span class="n">set_trainable</span> <span class="o">=</span> <span class="bp">True</span>
    <span class="k">if</span> <span class="n">set_trainable</span><span class="p">:</span>
        <span class="n">layer</span><span class="p">.</span><span class="n">trainable</span> <span class="o">=</span> <span class="bp">True</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">layer</span><span class="p">.</span><span class="n">trainable</span> <span class="o">=</span> <span class="bp">False</span>
</code></pre></div></div> <p>After the modification of the body of the model, in this case called <code class="language-plaintext highlighter-rouge">convbase</code>, we can add a head block as before so its weights will be adjusted during the training process as well.</p> <p>In the particular case of the repository listed at the top of this page, we can see that on the <a href="https://www.kaggle.com/c/dogs-vs-cats">Dogs vs Cats dataset</a> the accuracy of the model is improved from 81% of precision to 90% thanks to the use of the Transfer Learning technique. Furthermore, we went even further as, thanks to the fine tuning of our model, we obtained an accuracy of 95% of precision. This may be a good proof of how easy is to use Transfer Learning in a small project, where the access to the data is expensive or impossible, so it is one of the best tools to have in your hands while working on a personal project.</p> <p>Thank you very much for your attention and keep on learning!</p>]]></content><author><name></name></author><category term="deep learning"/><category term="transfer learning"/><category term="computer vision"/><category term="python"/><category term="tensorflow"/><summary type="html"><![CDATA[Guide to achieve state of the art performance on a wide range of tasks with little to no training.]]></summary></entry><entry><title type="html">Beyond Gradient Descent Optimizer</title><link href="https://parsaomidi.github.io/blog/2022/why-not-vanilla-gradient-descent/" rel="alternate" type="text/html" title="Beyond Gradient Descent Optimizer"/><published>2022-07-19T01:00:00+00:00</published><updated>2022-07-19T01:00:00+00:00</updated><id>https://parsaomidi.github.io/blog/2022/why-not-vanilla-gradient-descent</id><content type="html" xml:base="https://parsaomidi.github.io/blog/2022/why-not-vanilla-gradient-descent/"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/gradient-descent-01-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/gradient-descent-01-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/gradient-descent-01-1400.webp"/> <img src="/assets/img/gradient-descent-01.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>In almost every learning resource, books, references, or guides about Machine Learning is based on the optimization of a cost function by a learning algorithm.</p> <p>On almost every occasion, with the particular exception of the normal equation on linear regression problems, whenever we talk about optimization we are refering to the Gradient Descent algorithm. This algorithm is based on the idea of calculating the partial derivative of the different weights of the model with respect to the cost function in a point, which indicates the adjustment necessary to be performed in that weight to minimize the cost function. As the partial derivative is subject to the point in which the cost function is located, we multiply the value of the cost function by a learning rate to guide how much we will move the value of our weights.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/gradient-descent-02-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/gradient-descent-02-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/gradient-descent-02-1400.webp"/> <img src="/assets/img/gradient-descent-02.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Thus this learning rate parameter, often called $\alpha$ or simply <code class="language-plaintext highlighter-rouge">lr</code>, has a huge impact on the results of our model. If we decide to make our learning rate too low, we may need more steps for our model to converge, and if we decide to make it too big, our model may not converge at all.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/gradient-descent-03-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/gradient-descent-03-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/gradient-descent-03-1400.webp"/> <img src="/assets/img/gradient-descent-03.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>In simple models, we can allow a low value for this function since the training times are low, but in complicated models such as deep learning where the values to update are of order hundreds or thousands, we need alternatives:</p> <h2 id="stochastic-gradient-descent">Stochastic Gradient Descent</h2> <p>The very same word <em>stochastic</em> refers to the randomness of this method, which is based on selecting randomly K examples from our training set and computing the cost with them instead of with the total set, which allow us to perform updates in the weights as soon as possible and accelerate the training process. The problem of this method is its randomness, because if we had taken the pure gradient descent approach we would have taken into account all the inputs of data, but instead we are only considering a small subset of the data and adjusting the weights according to their loss, which could lead to different results and eventually lead to a non-optimal solution.</p> <h2 id="mini-batch-gradient-descent">Mini-batch Gradient Descent</h2> <p>The solution to the randomness problem on stochastic gradient descent is based on dividing our training set into K sets and adjusting the weights after each set, so that we are sure that our model will have to look at all the examples of the training set on each iteration. Each step of the full training set is called an epoch and this is a standard method in the automatic learning models.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/gradient-descent-04-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/gradient-descent-04-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/gradient-descent-04-1400.webp"/> <img src="/assets/img/gradient-descent-04.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="momentum-gradient-descent">Momentum Gradient Descent</h2> <p>Nevertheless, despite realizing faster adjustments thanks to the batch division of the dataset, we sometimes see that our cost function is not getting closer to the minimum, because the different sets of data may still give us results contrary to the desired result.</p> <p>In order to reduce the impact of this problem, the idea behind the momentum algorithm is to give a weight to the previous partial derivatives before performing our adjustment, which allows us to accelerate and smooth the adjustments to be made by our algorithm. The idea to follow is the same as in the smoothing of time series, reducing the impact of individual observations to get a tendency and directionality inside the function.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/gradient-descent-05-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/gradient-descent-05-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/gradient-descent-05-1400.webp"/> <img src="/assets/img/gradient-descent-05.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="rmsprop">RMSProp</h2> <p>On the next iteration, the <em>Root Mean Squared Propagation</em> method appears. It corrects the negative effects of the accumulation of the values passed through the derivatives, by converting them into a moving average adjusted. Furthermore, it allow us to select a different learning rate for each weight, which is a particularly good approach for deep learning models, where we can have thousands or millions of values to optimize.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/gradient-descent-06-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/gradient-descent-06-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/gradient-descent-06-1400.webp"/> <img src="/assets/img/gradient-descent-06.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="adam">ADAM</h2> <p>Finally, now that we now the basics of the momentum gradient descent and the RMSProp, we can combine the strenghts of both methods to get a more efficient optimizer that reduce the negative counterparts of selecting each of those. With this idea we reached the ADAM algorithm, which uses the quotient of the adjustments made by the two previous methods to get a more adjusted value, adding an epsilon parameter that allows to modify the weight of the momentum trend. In practice, this $/epsilon$ parameter is not modified and is left as it is on the default implementations used in libraries as <code class="language-plaintext highlighter-rouge">Keras</code> or <code class="language-plaintext highlighter-rouge">PyTorch</code>.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/gradient-descent-07-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/gradient-descent-07-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/gradient-descent-07-1400.webp"/> <img src="/assets/img/gradient-descent-07.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="final-remarks">Final remarks</h2> <p>Now that we have seen the different alternatives to vanilla gradient descent, it is easy to understand why we often find ourselves with one of the optimizers seen in this text more and more often. If even though our guide was not exhaustive, and there are still other optimizers to consider (such as Adagrad or Adadelta), in the practice each time more practitioners of automatic learning are enthused by ADAM, RMSProp or SGD, as those 3 are the most often seen in web competitions hosted in sites like <a href="www.kaggle.com">Kaggle</a>.</p> <p>If even after selecting an optimizer as ADAM or RMSProp you still find yourself with a sluggish model, you can consider the application of other optimization techniques such as Learning Rate Decay, which would allow us to reduce the learning rate according to the progress of the model in order to make large adjustments at the beginning and precise and small adjustments at the end:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/gradient-descent-08-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/gradient-descent-08-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/gradient-descent-08-1400.webp"/> <img src="/assets/img/gradient-descent-08.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The combination of this technique with the use of a more advanced optimizer like ADAM allows our models and networks to evolve faster towards the desired minimum of our cost function.</p> <p>I hope the explanation was clear and that your next models will use one of the optimizers explained here. Keep reading, practicing, and experimenting, and come back to share with me your advances!</p>]]></content><author><name></name></author><category term="deep learning"/><category term="optimization"/><summary type="html"><![CDATA[What are those fancy optimizers as ADAM or RMSprop and why should I use them instead of the old trusty SGD?]]></summary></entry></feed>